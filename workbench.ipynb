{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from scipy.stats import pearsonr as pearsonr\n",
    "from scipy.stats import skew as skew\n",
    "import math\n",
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize\n",
    "from scipy.stats.mstats import kruskalwallis as kruskalwallis\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def y_Ao(annotations):\n",
    "    total = 0.0\n",
    "    pairings = [x for x in itertools.combinations(annotations,2)]\n",
    "    for a1,a2 in pairings:\n",
    "        total+=int(a1==a2)\n",
    "    return total / len(pairings)\n",
    "\n",
    "\n",
    "def cleanstring(s):\n",
    "    if type(s) == float or type(s) == int:\n",
    "        if math.isnan(s):\n",
    "            return \"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = str(s).strip()\n",
    "    s = s.replace(\"&#44\",\", \")\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "class AnnotatedInstance:\n",
    "    def __init__(self):\n",
    "        self.leftcontext = ''\n",
    "        self.rightcontext = ''\n",
    "        self.headword = ''\n",
    "        self.labels = []\n",
    "        self.times = []\n",
    "        self.lemmafreq = 0\n",
    "                \n",
    "    def length(self):\n",
    "        return len((self.leftcontext+\" \"+self.rightcontext).split())\n",
    "    \n",
    "    def contentlength(self):\n",
    "        return len([x for x  in (self.leftcontext+\" \"+self.rightcontext).lower().split() if x not in stopwords.words('english')])\n",
    "    \n",
    "    def Ao(self):\n",
    "        return y_Ao(self.labels)\n",
    "    \n",
    "    def avg_time(self,threshold=100):\n",
    "        return [x for x in self.times if x < threshold]\n",
    "    \n",
    "    def max_time(self,threshold=100):\n",
    "        return max([x for x in self.times if x < threshold])\n",
    "\n",
    "    \n",
    "    def _normlabels(self):\n",
    "        d = {}\n",
    "        d['ANIM']='LIT'\n",
    "        d['Category1']='LIT'\n",
    "        \n",
    "        d['MEAT']='MET'\n",
    "        d['Categpory2']='MET'\n",
    "        d['Category2']='MET'\n",
    "        \n",
    "        d['DOT']='DOT'\n",
    "        d['Category4']='DOT'\n",
    "        d['Category3']='DOT'\n",
    "        return [d[x] for x in self.labels]\n",
    "    \n",
    "    def label(self):\n",
    "        #LIT, MET, DOT\n",
    "        normlabels = self._normlabels()\n",
    "        if normlabels.count('LIT') > normlabels.count('MET') and normlabels.count('DOT') <= normlabels.count('LIT'):\n",
    "            return 'LIT'\n",
    "        if normlabels.count('MET') > normlabels.count('LIT') and normlabels.count('DOT') <= normlabels.count('MET'):\n",
    "            return 'MET'\n",
    "        else:\n",
    "            return 'DOT'\n",
    "        \n",
    "    def __str__(self):\n",
    "        return '-'.join([i for i in [self.leftcontext,self.headword,self.rightcontext]+self.labels+[str(x) for x in self.times]])\n",
    "    \n",
    "    \n",
    "def CreateInstanceDict(dataframe,freqdict):\n",
    "    instancedict = {}\n",
    "    for hitid in set(dataframe.Input_globalindex):\n",
    "        inst = AnnotatedInstance()\n",
    "        inst.times = list(dataframe[dataframe.Input_globalindex==hitid].WorkTimeInSeconds)\n",
    "        inst.labels = list(dataframe[dataframe.Input_globalindex==hitid].Answer_Category)\n",
    "        inst.leftcontext = cleanstring(list(dataframe[dataframe.Input_globalindex==hitid].Input_lleftcontext)[0])\n",
    "        inst.rightcontext = cleanstring(list(dataframe[dataframe.Input_globalindex==hitid].Input_lrightcontext)[0])\n",
    "        inst.headword = cleanstring(list(dataframe[dataframe.Input_globalindex==hitid].Input_lheadword)[0])\n",
    "        inst.lemma = cleanstring(list(dataframe[dataframe.Input_globalindex==hitid].Input_lemma)[0]).lower()\n",
    "        inst.lemmafreq = math.log(sum(list(freqdict[freqdict.Lemma==inst.lemma].Freq)))\n",
    "        instancedict[hitid]=inst\n",
    "    return instancedict\n",
    "    \n",
    "\n",
    "def Datasettime(instancedict):\n",
    "    t =[]\n",
    "    for k in instancedict:\n",
    "        t.extend(instancedict[k].times)\n",
    "    return t\n",
    "    \n",
    "def ReportDataset(instancedict):\n",
    "    alltimes = Datasettime(instancedict)\n",
    "    percentile95 =  np.percentile(alltimes, 95) #right-hand outliers filtered, remove anything above the 95th percentile\n",
    "    alltimes= [x for x in alltimes if x < percentile95]\n",
    "    mean = np.mean(alltimes)\n",
    "    median = np.median(alltimes)\n",
    "    std = np.std(alltimes)\n",
    "    Ao_indiv_list = [instancedict[k].Ao() for k in sorted(instancedict.keys())]\n",
    "    Ao_avg = np.mean(Ao_indiv_list)\n",
    "    lemmafreqs =  [instancedict[k].lemmafreq for k in sorted(instancedict.keys())]\n",
    "    time_indiv_list =  [np.mean(instancedict[k].avg_time(percentile95)) for k in sorted(instancedict.keys())]\n",
    "    #time_indiv_list =  [instancedict[k].max_time(percentile95) for k in sorted(instancedict.keys())]\n",
    "\n",
    "    lengths = [instancedict[k].length() for k in sorted(instancedict.keys())]\n",
    "    contentlengths = [instancedict[k].contentlength() for k in sorted(instancedict.keys())]\n",
    "    labels = [instancedict[k].label() for k in sorted(instancedict.keys())]\n",
    "    #return (mean, median, std,percentile95,Ao_avg,pearsonr(Ao_indiv_list,time_indiv_list),pearsonr(lenghts,time_indiv_list)[0],Counter(labels)['LIT'])\n",
    "    return (pearsonr(Ao_indiv_list,time_indiv_list),pearsonr(lengths,time_indiv_list),pearsonr(Ao_indiv_list,contentlengths),pearsonr(time_indiv_list,contentlengths))\n",
    "    #return (' & '.join([str(x) for x in [Ao_avg,\"_\", np.mean(lengths),np.mean(alltimes), np.median(alltimes), np.std(alltimes), np.mean(lemmafreqs)]]))\n",
    "\n",
    "\n",
    "\n",
    "def getLiteral(inputlist):\n",
    "    litlabels = ['Category1', 'ANIM']\n",
    "    return [int(x in litlabels) for x in inputlist]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "animeat = pd.read_csv('data/raw/animeatbatchresults.csv')\n",
    "contcont = pd.read_csv('data/raw/container_5turks.tab.csv',sep='\\t')\n",
    "artinfo = pd.read_csv('data/raw/artinfo_mturk_500.tab.csv',sep='\\t')\n",
    "procres = pd.read_csv('data/raw/procres_mturk.tab.csv',sep='\\t')\n",
    "locorg = pd.read_csv('data/raw/locorg_5turks.tab.csv',sep='\\t')\n",
    "freqdict =  pd.read_csv('data/ANC-all-lemma.txt',sep='\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animeat & ((-0.37962472276825493, 6.2812478301620924e-19), (0.30349345562040358, 2.507678312321528e-12), (-0.090502627882570269, 0.041050833789383491), (0.31124205750215744, 6.4336911842716168e-13))\n",
      "artinfo & ((-0.077477910735849531, 0.080458386793397715), (0.39617819840821344, 1.2949504682619278e-20), (-0.068731545884461051, 0.12109308343671212), (0.3707587474279373, 4.5938874944703922e-18))\n",
      "contcont & ((-0.074817980159857772, 0.091121674199407587), (0.052722076290081819, 0.23416128805212155), (-0.032213689079616983, 0.46746755368196491), (0.066052680406299399, 0.13593172510299137))\n",
      "locorg & ((-0.060481137096010759, 0.17264513106924814), (0.063840031407057574, 0.14996867701964953), (-0.024121064635798068, 0.58680432688642281), (0.058807605728609214, 0.18485568626889542))\n",
      "procres & ((-0.07320787067266385, 0.09865156203155169), (0.10057235289900357, 0.023120827193312352), (0.061421324872990436, 0.1660545633348231), (0.11471706578419408, 0.009517481007490617))\n"
     ]
    }
   ],
   "source": [
    "datataframes = [animeat, artinfo, contcont, locorg, procres]\n",
    "names = ['animeat', 'artinfo', 'contcont', 'locorg','procres']\n",
    "instancedicts = {}\n",
    "for dataframe,name in zip(datataframes,names):\n",
    "    instancedicts[name] = CreateInstanceDict(dataframe,freqdict)\n",
    "    \n",
    "for k in sorted(instancedicts.keys()):\n",
    "    print(k,'&',ReportDataset(instancedicts[k]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From here we can see that all datasets have a different ... AND very different mean annotation time.'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"From here we can see that all datasets have a different ... AND very different mean annotation time.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def TimeStochasticDominanceReport(instancedict):\n",
    "    lit_times = [instancedict[k].length() for k in sorted(instancedict.keys()) if instancedict[k].label() == 'LIT']\n",
    "    met_times = [instancedict[k].length() for k in sorted(instancedict.keys()) if instancedict[k].label() == 'MET']\n",
    "    dot_times = [instancedict[k].length() for k in sorted(instancedict.keys()) if instancedict[k].label() == 'DOT']\n",
    "    \n",
    "    return(kruskalwallis(dot_times,lit_times,met_times)[1],kruskalwallis(dot_times,lit_times)[1],kruskalwallis(dot_times,met_times)[1],kruskalwallis(lit_times,met_times)[1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animeat (0.094369166647093439, 0.37558382851465788, 0.1855649967594365, 0.054435061456270514)\n",
      "artinfo (0.035283678067990855, 0.2834837963286807, 0.58772734602222521, 0.0093734488583511709)\n",
      "contcont (0.10580686686326722, 0.2815205483439136, 0.079521362729517875, 0.091201095701691798)\n",
      "locorg (0.98199869027376896, 0.89127426077502103, 0.86604148780064705, 0.90698976456014957)\n",
      "procres (0.70951161264087814, 0.79879402487028772, 0.55721729259553077, 0.49240651278298209)\n"
     ]
    }
   ],
   "source": [
    "for k in sorted(instancedicts.keys()):\n",
    "    print(k,TimeStochasticDominanceReport(instancedicts[k]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
